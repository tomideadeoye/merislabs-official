{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the preferred synthesizer model explicitly\n",
        "SYNTHESIZER_PROVIDER = 'gemini'\n",
        "SYNTHESIZER_MODEL_ID = 'gemini/gemini-2.5-pro-exp' # Make sure this matches a config in PROVIDER_MODEL_CONFIGS['gemini']\n",
        "DEFAULT_GENERATION_PROVIDERS = [\"groq\", \"azure\", \"gemini\", \"mistral\", \"cohere\", \"openrouter\"]\n",
        "\n",
        "PROVIDER_MODEL_CONFIGS = {\n",
        "\n",
        "           'groq': [\n",
        "        {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'groq/llama-3.3-70b-versatile', # Recommended replacement for Llama 3.1 and Tool Use models\n",
        "            'api_key_env': 'GROQ_API_KEY',\n",
        "            # '_comment': 'Current flagship Llama 3.3 model on Groq.'\n",
        "        },\n",
        "        {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'groq/mistral-saba-24b', # Recommended replacement for Mixtral 8x7B\n",
        "            'api_key_env': 'GROQ_API_KEY',\n",
        "             # '_comment': 'Current recommended Mistral family model on Groq.'\n",
        "        },\n",
        "         {\n",
        "             'model_class': LiteLLMModel,\n",
        "             'model_id': 'groq/deepseek-r1-distill-qwen-32b', # Recommended reasoning model, replaced specdec Llama\n",
        "             'api_key_env': 'GROQ_API_KEY',\n",
        "              # '_comment': 'Current recommended Deepseek/Qwen based reasoning model.'\n",
        "          },\n",
        "         {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'groq/gemma2-9b-it', # Recommended replacement for Gemma 7B\n",
        "            'api_key_env': 'GROQ_API_KEY',\n",
        "             # '_comment': 'Current recommended Gemma model on Groq.'\n",
        "         },\n",
        "        # { # Optional: Include the non-specdec Deepseek Llama 70B if needed\n",
        "        #     'model_class': LiteLLMModel,\n",
        "        #     'model_id': 'groq/deepseek-r1-distill-llama-70b', # Replacement for its specdec version\n",
        "        #     'api_key_env': 'GROQ_API_KEY',\n",
        "        # },\n",
        "        # { # Optional: Include the Llama 3.3 specdec version if needed\n",
        "        #     'model_class': LiteLLMModel,\n",
        "        #     'model_id': 'groq/llama-3.3-70b-specdec', # Replacement for Llama 3.1 specdec\n",
        "        #     'api_key_env': 'GROQ_API_KEY',\n",
        "        # },\n",
        "        # { # Optional: Older, smaller model mentioned as replacement\n",
        "        #    'model_class': LiteLLMModel,\n",
        "        #    'model_id': 'groq/llama-3.1-8b-instant',\n",
        "        #    'api_key_env': 'GROQ_API_KEY',\n",
        "        #    '_comment': 'Mentioned as replacement, but likely older than Llama 3.3.'\n",
        "        # }\n",
        "\n",
        "    ],\n",
        "    'azure': [\n",
        "        # --- Azure OpenAI Models ---\n",
        "        # { # Example for GPT-4.5 Preview (Requires Registration/Access)\n",
        "        #     'model_class': AzureOpenAIServerModel,\n",
        "        #     'model_id': 'gpt-45-preview', # Replace with YOUR specific deployment name for GPT-4.5\n",
        "        #     'azure_endpoint_env': 'AZURE_AI_ENDPOINT',\n",
        "        #     'api_key_env': 'AZURE_API_KEY',\n",
        "        #     'api_version': '2024-08-06-preview', # Use API version supporting the preview model\n",
        "        # },\n",
        "        {\n",
        "            'model_class': AzureOpenAIServerModel,\n",
        "            'model_id': 'gpt-4o', # Replace with YOUR specific deployment name for GPT-4o\n",
        "            'azure_endpoint_env': 'AZURE_AI_ENDPOINT',\n",
        "            'api_key_env': 'AZURE_API_KEY',\n",
        "            'api_version': '2024-08-06', # Use a recent stable or preview API version\n",
        "        },\n",
        "    ],\n",
        "    'gemini': [\n",
        "         # --- Google Gemini Models ---\n",
        "         { # Experimental - Potentially Unstable\n",
        "           'model_class': LiteLLMModel,\n",
        "           'model_id': 'gemini/gemini-2.0-pro', # Latest Experimental Gemini\n",
        "           'api_key_env': 'GEMINI_API_KEY',\n",
        "           'temperature': 0.5, # Might need higher temp for experimental reasoning\n",
        "           'custom_llm_provider': 'gemini',\n",
        "         },\n",
        "        {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'gemini/gemini-1.5-pro-latest', # Top-tier stable Gemini, great for synthesis, large context\n",
        "            'api_key_env': 'GEMINI_API_KEY',\n",
        "            'temperature': 0.4,\n",
        "            'custom_llm_provider': 'gemini',\n",
        "        },\n",
        "        {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'gemini/gemini-2.0-flash-latest', # Fast Gemini, great for drafting stage\n",
        "            'api_key_env': 'GEMINI_API_KEY',\n",
        "            'temperature': 0.3,\n",
        "            'custom_llm_provider': 'gemini',\n",
        "        }\n",
        "    ],\n",
        "    'openrouter': [\n",
        "        # --- Top Tier Free Options ---\n",
        "        {\n",
        "            'model_class': LiteLLMModel,\n",
        "            # NOTE: Use the base model ID. OpenRouter handles the ':free' routing.\n",
        "            'model_id': 'openrouter/google/gemini-2.5-pro-exp-03-25', # Latest Google Experimental, 1M context\n",
        "            'api_key_env': 'OPEN_ROUTER_API_KEY',\n",
        "            'api_base': 'https://openrouter.ai/api/v1',\n",
        "            'temperature': 0.5, # Optional: Adjust temp for experimental model\n",
        "            # '_comment': 'Marked as free in user examples. Most capable free option listed.'\n",
        "        },\n",
        "        {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'openrouter/deepseek/deepseek-chat-v3-0324', # Flagship DeepSeek V3 Chat, 131K context\n",
        "            'api_key_env': 'OPEN_ROUTER_API_KEY',\n",
        "            'api_base': 'https://openrouter.ai/api/v1',\n",
        "            'temperature': 0.3,\n",
        "            # '_comment': 'Marked as free in user examples. Strong chat model.'\n",
        "        },\n",
        "        {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'openrouter/mistralai/mistral-nemo', # Mistral/NVIDIA 12B model, 128k context\n",
        "            'api_key_env': 'OPEN_ROUTER_API_KEY',\n",
        "            'api_base': 'https://openrouter.ai/api/v1',\n",
        "            'temperature': 0.3,\n",
        "            # '_comment': 'Commonly available free Mistral model on OR.'\n",
        "         },\n",
        "         {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'openrouter/meta-llama/llama-3.1-8b-instruct', # Small, efficient Llama 3.1\n",
        "            'api_key_env': 'OPEN_ROUTER_API_KEY',\n",
        "            'api_base': 'https://openrouter.ai/api/v1',\n",
        "            'temperature': 0.6, # Often benefits from slightly higher temp\n",
        "            # '_comment': 'Small Llama 3 models are frequently free on OR. Verify current status.'\n",
        "         },\n",
        "\n",
        "        # --- Other Free Options from User List ---\n",
        "        {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'openrouter/deepseek/deepseek-r1', # Deepseek R1 line\n",
        "            'api_key_env': 'OPEN_ROUTER_API_KEY',\n",
        "            'api_base': 'https://openrouter.ai/api/v1',\n",
        "            'temperature': 0.4,\n",
        "             # '_comment': 'Marked as free in user examples.'\n",
        "         },\n",
        "         {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'openrouter/google/gemini-2.0-flash-exp', # Older experimental Flash\n",
        "            'api_key_env': 'OPEN_ROUTER_API_KEY',\n",
        "            'api_base': 'https://openrouter.ai/api/v1',\n",
        "            'temperature': 0.4,\n",
        "             # '_comment': 'Marked as free in user examples.'\n",
        "          },\n",
        "         {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'openrouter/deepseek/deepseek-chat', # Older Deepseek Chat\n",
        "            'api_key_env': 'OPEN_ROUTER_API_KEY',\n",
        "            'api_base': 'https://openrouter.ai/api/v1',\n",
        "            'temperature': 0.3,\n",
        "            # '_comment': 'Marked as free in user examples. Fallback option.'\n",
        "         },\n",
        "    ],\n",
        "     'mistral': [\n",
        "              {\n",
        "            'model_class': LiteLLMModel,\n",
        "            # LiteLLM uses 'mistral/' prefix for direct API calls\n",
        "            'model_id': 'mistral/open-mistral-nemo', # Explicitly open source model with API endpoint\n",
        "            'api_key_env': 'MISTRAL_API_KEY',\n",
        "            'temperature': 0.3,\n",
        "            # '_comment': 'Best multilingual open source model listed with API. Direct API call likely paid.'\n",
        "         },\n",
        "         {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'mistral/mistral-large-latest', # Mistral's flagship reasoning model\n",
        "            'api_key_env': 'MISTRAL_API_KEY',\n",
        "         },\n",
        "         {\n",
        "            'model_class': LiteLLMModel,\n",
        "            'model_id': 'mistral/mistral-small-3.1', # Latest small/fast model (March 2025)\n",
        "            'api_key_env': 'MISTRAL_API_KEY',\n",
        "         },\n",
        "        # { # Optional: If coding is a primary task\n",
        "        #    'model_class': LiteLLMModel,\n",
        "        #    'model_id': 'mistral/codestral-latest',\n",
        "        #    'api_key_env': 'MISTRAL_API_KEY',\n",
        "        # }\n",
        "     ],\n",
        "     'cohere': [\n",
        "         # --- Cohere Models ---\n",
        "         {\n",
        "             'model_class': LiteLLMModel,\n",
        "             'model_id': 'cohere/command-a', # Latest flagship (March 2025)\n",
        "             'api_key_env': 'COHERE_API_KEY',\n",
        "             # 'temperature': 0.3\n",
        "         },\n",
        "         {\n",
        "             'model_class': LiteLLMModel,\n",
        "             'model_id': 'cohere/command-r-plus', # Previous flagship, still very capable\n",
        "             'api_key_env': 'COHERE_API_KEY',\n",
        "         },\n",
        "         # { # Optional: If multilingual/vision needed (Non-commercial license)\n",
        "         #     'model_class': LiteLLMModel,\n",
        "         #     'model_id': 'cohere/aya-vision-35b',\n",
        "         #     'api_key_env': 'COHERE_API_KEY',\n",
        "         # }\n",
        "     ],\n",
        "     'together_ai': [\n",
        "         # --- Models via Together AI Platform ---\n",
        "         {\n",
        "             'model_class': LiteLLMModel,\n",
        "             'model_id': 'together_ai/meta-llama/Llama-3.1-70B-Instruct-hf', # Llama 3.1 70B via Together\n",
        "             'api_key_env': 'TOGETHER_API_KEY',\n",
        "         },\n",
        "         {\n",
        "             'model_class': LiteLLMModel,\n",
        "             'model_id': 'together_ai/Qwen/Qwen2-72B-Instruct', # Qwen 2 72B via Together\n",
        "             'api_key_env': 'TOGETHER_API_KEY',\n",
        "         },\n",
        "         {\n",
        "              'model_class': LiteLLMModel,\n",
        "              'model_id': 'together_ai/deepseek-ai/deepseek-coder-v2-instruct', # Strong coder via Together\n",
        "              'api_key_env': 'TOGETHER_API_KEY',\n",
        "          }\n",
        "     ],\n",
        "    # Add other providers like Cloudflare if needed, ensuring correct model IDs and auth params\n",
        "    # 'cloudflare': [...]\n",
        "         # mistral\n",
        "# openrouter (deepseek/deepseek-r1:free, google/gemini-2.0-pro-exp-02-05:free, deepseek/deepseek-chat-v3-0324:free, google/gemini-2.0-flash-lite-preview-02-05:free)\n",
        "# huggingchat\n",
        "# groq\n",
        "# gemini\n",
        "# azure\n",
        "# Mistral (Codestral)\n",
        "# Cerebras\n",
        "# OVH AI Endpoints (Free Beta)\n",
        "# Together\n",
        "# Cohere\n",
        "# GitHub Models\n",
        "# Cloudflare Workers AI\n",
        "}\n",
        "\n",
        "\n",
        "class TimeoutException(Exception): pass\n",
        "\n",
        "@contextmanager\n",
        "def time_limit(seconds: int):\n",
        "    def signal_handler(signum, frame):\n",
        "        raise TimeoutException(f\"Operation timed out after {seconds} seconds\")\n",
        "\n",
        "    if not hasattr(signal, 'SIGALRM'):\n",
        "        logging.warning(\"signal.SIGALRM not available on this platform. Timeout will not be enforced.\")\n",
        "        try: yield\n",
        "        finally: return\n",
        "\n",
        "    original_handler = signal.signal(signal.SIGALRM, signal_handler)\n",
        "    try:\n",
        "        signal.alarm(seconds); yield\n",
        "    finally:\n",
        "        signal.alarm(0); signal.signal(signal.SIGALRM, original_handler)\n",
        "\n",
        "def create_model_instance(model_config: dict) -> Optional[Any]:\n",
        "    params = {}\n",
        "    required_keys = ['model_class', 'model_id']\n",
        "    if not all(key in model_config for key in required_keys):\n",
        "        logging.error(f\"Model config missing required keys ({required_keys}): {model_config}\"); return None\n",
        "\n",
        "    ModelClass = model_config['model_class']\n",
        "    model_id = model_config['model_id']\n",
        "    class_name = ModelClass.__name__\n",
        "\n",
        "    try:\n",
        "        for key, value in model_config.items():\n",
        "            if key in required_keys: continue\n",
        "            if key.endswith('_env'):\n",
        "                env_var_name = value\n",
        "                env_value = os.getenv(env_var_name)\n",
        "                param_name = key[:-4]\n",
        "                if not env_value:\n",
        "                    essential_keys = ['api_key', 'azure_endpoint']\n",
        "                    is_essential = param_name in essential_keys or \\\n",
        "                                   (param_name == 'api_key' and class_name == 'LiteLLMModel' and not model_id.startswith(\"ollama\"))\n",
        "                    if is_essential:\n",
        "                         logging.warning(f\"Missing essential env var '{env_var_name}' for {model_id}. Skipping.\"); return None\n",
        "                    else:\n",
        "                         logging.debug(f\"Optional env var '{env_var_name}' not set for {model_id}.\"); params[param_name] = None\n",
        "                else: params[param_name] = env_value\n",
        "            else: params[key] = value\n",
        "\n",
        "        logging.debug(f\"Creating model: {class_name}(model_id='{model_id}', **{params})\")\n",
        "        # Pass all collected params during instantiation\n",
        "        model_instance = ModelClass(model_id=model_id, **params)\n",
        "        logging.debug(f\"Created instance for model '{model_id}'\")\n",
        "        return model_instance\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error creating model instance for '{model_id}': {e}\", exc_info=True); return None\n",
        "\n",
        "\n",
        "def llm_request( query: str, providers: List[str] = DEFAULT_GENERATION_PROVIDERS, return_all: bool = False, timeout: int = 60) -> Dict[str, str]:\n",
        "    logging.info(f\"Attempting LLM request with providers: {providers}\")\n",
        "    results = {}; processed_providers = set()\n",
        "    for provider_name in providers:\n",
        "        provider_name = provider_name.lower()\n",
        "        if provider_name in processed_providers: continue\n",
        "        processed_providers.add(provider_name)\n",
        "        if provider_name not in PROVIDER_MODEL_CONFIGS:\n",
        "            logging.warning(f\"Provider '{provider_name}' not configured.\"); continue\n",
        "\n",
        "        candidate_configs = PROVIDER_MODEL_CONFIGS[provider_name]\n",
        "        for config in candidate_configs:\n",
        "            model_id_original = config.get('model_id', 'unknown_model')\n",
        "            result_key = f\"{provider_name}_{model_id_original.replace('/', '_')}\"\n",
        "            logging.info(f\"Attempting provider: '{provider_name}', model: '{model_id_original}'\")\n",
        "\n",
        "            # Pass the full config dict to create_model_instance\n",
        "            model_instance = create_model_instance(config) # Pass the whole config dict\n",
        "            if not model_instance:\n",
        "                logging.warning(f\"Skipping model '{model_id_original}' due to creation failure.\"); continue\n",
        "\n",
        "            try:\n",
        "                with time_limit(timeout):\n",
        "                    agent = CodeAgent(tools=[], model=model_instance)\n",
        "                    response = agent.run(query)\n",
        "                if response:\n",
        "                    response_text = str(response)\n",
        "                    logging.info(f\"SUCCESS with provider '{provider_name}', model '{model_id_original}'.\")\n",
        "                    results[result_key] = response_text\n",
        "                    if not return_all: return results\n",
        "                    else: break # Success for this provider, move to next if return_all=True\n",
        "                else: logging.warning(f\"Provider '{provider_name}', model '{model_id_original}' returned empty response.\")\n",
        "            except TimeoutException as e: logging.error(f\"TIMEOUT: {provider_name} {model_id_original}: {e}\")\n",
        "            except Exception as e: logging.error(f\"ERROR: {provider_name} {model_id_original}: {e}\", exc_info=True)\n",
        "\n",
        "    if not results: logging.warning(\"No successful responses received.\"); return {'error': \"No successful responses.\"}\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "# --- Constants and Helper for get_llm_answer ---\n",
        "BROWSER_CONTEXT_MAX_CHARS = 5000\n",
        "MIN_DRAFT_LENGTH = 25 # Minimum characters for a draft to be considered potentially valid\n",
        "\n",
        "def is_valid_response(response: Optional[str]) -> bool:\n",
        "    \"\"\"Checks if a response string looks like a valid answer (not None, error, or too short).\"\"\"\n",
        "    if response is None or len(response) < MIN_DRAFT_LENGTH:\n",
        "        return False\n",
        "    response_lower = response.lower()\n",
        "    error_indicators = [\n",
        "        \"error:\", \"fail\", \"timeout\", \"cannot process\", \"unable to handle\",\n",
        "        \"instance creation failed\", \"returned none\", \"empty response\"\n",
        "    ]\n",
        "    if any(indicator in response_lower for indicator in error_indicators):\n",
        "         return False\n",
        "    # Add more sophisticated checks if needed (e.g., check for repetitive patterns)\n",
        "    return True\n",
        "\n",
        "def get_llm_answer(\n",
        "    request_type: str,\n",
        "    primary_context: str,\n",
        "    profile_context: str,\n",
        "    browser_context: Optional[str] = None,\n",
        "    question: str = \"\",\n",
        "    generation_providers: List[str] = DEFAULT_GENERATION_PROVIDERS,\n",
        "    llm_timeout: int = 60,\n",
        "    synthesizer_timeout: int = 90 # Allow more time for synthesis\n",
        ") -> Tuple[Dict[str, str], Optional[str]]:\n",
        "\n",
        "    logging.info(f\"Starting 2-stage LLM process. Request type: '{request_type}'. Stage 1 Providers: {generation_providers}\")\n",
        "\n",
        "    # --- Stage 0: Prepare Initial Prompt for Drafting ---\n",
        "    logging.debug(\"Preparing Stage 1 prompt...\")\n",
        "    safe_browser_context = (browser_context or \"\")[:BROWSER_CONTEXT_MAX_CHARS]\n",
        "    task_descriptions = {\n",
        "        'email': \"Write a personalized job application email using data from the Personal Information context and integrating relevant information from the Online Information context.\",\n",
        "        'cover_letter': \"Write a detailed, personalized cover letter specific to the Primary Context, using relevant data from the Personal Information context and incorporating useful insights from the Online Information context.\",\n",
        "        'question': \"Answer the Question in detail using data from the Personal Information context, supplemented with relevant information from the Primary Context and Online Information context.\",\n",
        "        'intro_email': \"Write a personalized introductory email to a potential employer, business partner, or networking contact (details in Primary Context). Use data from the Personal Information context, integrating relevant information from the Online Information context. Draft a tailored partnership proposal/collaboration initiation, highlighting alignment between Personal Information and Primary Context objectives. Communicate interest in joining projects, teams, or networks based on shared expertise and alignment.\",\n",
        "        'linkedin_message': \"Compose a concise and professional LinkedIn message to introduce yourself to a potential collaborator or mentor (details in Primary Context). Use data from the Personal Information context and relevant details from the Online Information context.\",\n",
        "        'networking_followup': \"Compose a follow-up email after a networking interaction (details likely in Primary Context). Reference specific points from the previous conversation and propose next steps, using the Personal Information context for background.\",\n",
        "        'mentorship_request': \"Craft a personalized email requesting mentorship from a professional (details in Primary Context). Use data from the Personal Information context and relevant Online Information. Emphasize eagerness to learn, contribute to their projects/team, and build a mutually beneficial relationship.\"\n",
        "    }\n",
        "    task = task_descriptions.get(request_type, f\"Respond appropriately to the request '{request_type}' based on the provided contexts, prioritizing the Personal Information.\")\n",
        "\n",
        "    # Keep the detailed instructions for the drafters\n",
        "    stage1_prompt = f\"\"\"You are an AI assistant tasked with generating a draft for a specific communication goal. Your goal is to craft a personalized, impactful communication tailored to a specific opportunity, strictly adhering to the provided context and instructions.\n",
        "\n",
        "**Task:** {task}\n",
        "\n",
        "**Primary Context:** {primary_context}\n",
        "\n",
        "**Personal Information (Your Profile):** {profile_context}\n",
        "\n",
        "**Online Information (Supplementary Context):** {safe_browser_context if safe_browser_context else \"N/A\"}\n",
        "\n",
        "**Specific Question (if applicable):** {question if question else \"N/A\"}\n",
        "\n",
        "**Instructions for Drafting:**\n",
        "1.  **Focus:** Accurately reflect the Personal Information in relation to the Primary Context. Align skills, experiences, and values from the profile with the **Primary Context**.\n",
        "2.  **Context Usage:** Use Online Information sparingly for enhancement only, if relevant.\n",
        "3.  **Adherence:** Strictly adhere to the core requirements of the Task (e.g., tone, purpose).\n",
        "4.  **Tailoring:** Make the draft highly specific to the **Primary Context** (recipient, company, role, opportunity). Avoid generic language. Incorporate industry-specific terminology where appropriate.\n",
        "5.  **Unique Selling Points:** Highlight relevant accomplishments, projects, responsibilities, values, and soft skills from the profile. Use measurable outcomes if available.\n",
        "6.  **Psychological Principles (Subtle):** Consider incorporating principles like Reciprocity (offer value), Liking (common ground), Authority (state expertise), Storytelling (brief narrative if relevant), Consistency (align with recipient's goals).\n",
        "7.  **Tone:** Maintain a balance between professional enthusiasm and authenticity. Be direct and confident.\n",
        "8.  **Clarity & Conciseness:** Be brief but ensure every sentence adds value.\n",
        "9.  **Call to Action:** If appropriate for the task, include a clear, specific suggestion for next steps.\n",
        "10. **Mentorship Specifics:** If `request_type` is 'mentorship_request', explicitly state the desire to learn, contribute, and potential mutual benefits.\n",
        "\n",
        "**Generate ONLY the draft communication based on the above:**\n",
        "\"\"\"\n",
        "\n",
        "    # --- Stage 1: Generate Drafts ---\n",
        "    logging.info(\"--- Starting Stage 1: Draft Generation ---\")\n",
        "    stage1_results = {}\n",
        "    try:\n",
        "        stage1_results = llm_request(\n",
        "            query=stage1_prompt,\n",
        "            providers=generation_providers,\n",
        "            return_all=True, # Get drafts from all providers\n",
        "            timeout=llm_timeout\n",
        "        )\n",
        "        logging.info(f\"Stage 1 finished. Received {len(stage1_results)} raw results.\")\n",
        "        if 'error' in stage1_results and len(stage1_results) == 1:\n",
        "             logging.error(f\"Stage 1 failed completely: {stage1_results['error']}\")\n",
        "             return stage1_results, None # Return error dict and None for synthesis\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Critical error during Stage 1 execution: {e}\", exc_info=True)\n",
        "        return {'error': f\"Critical failure in Stage 1: {e}\"}, None # Return error dict\n",
        "\n",
        "    # --- Stage 1.5: Filter and Prepare Drafts for Synthesis ---\n",
        "    valid_drafts = {}\n",
        "    draft_count = 0\n",
        "    formatted_drafts_for_prompt = \"\"\n",
        "    for key, draft in stage1_results.items():\n",
        "        if key == 'error': continue # Skip the overall error message if present\n",
        "        if is_valid_response(draft):\n",
        "            draft_count += 1\n",
        "            valid_drafts[key] = draft\n",
        "            formatted_drafts_for_prompt += f\"\\n\\n--- Draft {draft_count} (from {key}) ---\\n{draft}\\n--- End Draft {draft_count} ---\"\n",
        "            logging.debug(f\"Added valid draft from {key} (len={len(draft)})\")\n",
        "        else:\n",
        "            logging.warning(f\"Invalid draft from {key} excluded from synthesis.\")\n",
        "\n",
        "    if not valid_drafts:\n",
        "        logging.error(\"Stage 1 did not produce any valid drafts for synthesis.\")\n",
        "        # Return the raw stage1 results (which might contain errors) and None for synthesis\n",
        "        return stage1_results, None\n",
        "\n",
        "    logging.info(f\"Proceeding to Stage 2 with {len(valid_drafts)} valid drafts.\")\n",
        "\n",
        "    # --- Stage 2: Synthesize Drafts ---\n",
        "    logging.info(\"--- Starting Stage 2: Synthesis ---\")\n",
        "    final_response: Optional[str] = None\n",
        "\n",
        "    # Prepare the synthesizer prompt\n",
        "    stage2_prompt = f\"\"\"You are a Synthesizer AI. Your task is to analyze multiple draft responses generated for a specific communication goal and create a single, cohesive, improved final version.\n",
        "\n",
        "**Original Task Goal:** {task}\n",
        "\n",
        "**Original Primary Context:** {primary_context}\n",
        "\n",
        "**Original Personal Information (User Profile):** {profile_context}\n",
        "\n",
        "**Original Online Information (Supplementary Context):** {safe_browser_context if safe_browser_context else \"N/A\"}\n",
        "\n",
        "**Original Specific Question (if applicable):** {question if question else \"N/A\"}\n",
        "\n",
        "**Instructions for Synthesis:**\n",
        "1.  **Analyze Drafts:** Carefully review all the provided drafts below.\n",
        "2.  **Identify Strengths:** Extract the best elements, strongest phrases, most relevant points, and unique insights from *each* draft.\n",
        "3.  **Harmonize:** Combine these elements into a single, coherent, and well-structured response. Ensure smooth transitions and logical flow.\n",
        "4.  **Improve:** Enhance the combined text for clarity, conciseness, tone, and impact. Correct any grammatical errors or awkward phrasing.\n",
        "5.  **Adhere to Original Goal:** Ensure the final synthesized output fully addresses the **Original Task Goal** and respects all constraints mentioned in the initial drafting instructions (e.g., personalization, context usage, tone, call to action).\n",
        "6.  **Do Not Just Copy:** Create a *new*, superior version based on the input drafts; do not simply copy one draft entirely.\n",
        "7.  **Output:** Provide *only* the final, synthesized communication. Do not include introductions like \"Here is the synthesized version:\".\n",
        "\n",
        "**Draft Responses to Synthesize:**\n",
        "{formatted_drafts_for_prompt}\n",
        "\n",
        "**Generate the single, final, synthesized communication based ONLY on the analysis of the drafts and the original context/goal:**\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        logging.info(f\"Attempting synthesis using {SYNTHESIZER_PROVIDER} model {SYNTHESIZER_MODEL_ID}.\")\n",
        "        # Use llm_request to call the specific synthesizer model\n",
        "        synthesis_result = llm_request(\n",
        "            query=stage2_prompt,\n",
        "            providers=[SYNTHESIZER_PROVIDER], # Target only the synthesizer provider\n",
        "            return_all=False, # We only need one response from the synthesizer\n",
        "            timeout=synthesizer_timeout\n",
        "        )\n",
        "\n",
        "        if synthesis_result and 'error' not in synthesis_result:\n",
        "            # Get the first (and only) response value\n",
        "            final_response = next(iter(synthesis_result.values()))\n",
        "            if is_valid_response(final_response):\n",
        "                 logging.info(f\"Stage 2 Synthesis successful. Final response length: {len(final_response)}\")\n",
        "                 logging.debug(f\"Final response snippet: {final_response[:150]}...\")\n",
        "            else:\n",
        "                 logging.warning(\"Synthesizer produced an invalid or too short response.\")\n",
        "                 final_response = None # Discard invalid synthesis\n",
        "                 # Add the synthesis error/invalid response to the stage1 results for context\n",
        "                 stage1_results['synthesis_error'] = f\"Synthesizer response from {SYNTHESIZER_MODEL_ID} was invalid or too short.\"\n",
        "\n",
        "        elif synthesis_result and 'error' in synthesis_result:\n",
        "            logging.error(f\"Stage 2 Synthesis failed: {synthesis_result['error']}\")\n",
        "            stage1_results['synthesis_error'] = synthesis_result['error'] # Add error info\n",
        "        else:\n",
        "             logging.error(\"Stage 2 Synthesis failed: No response received from synthesizer.\")\n",
        "             stage1_results['synthesis_error'] = \"No response received from synthesizer.\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Critical error during Stage 2 Synthesis execution: {e}\", exc_info=True)\n",
        "        stage1_results['synthesis_error'] = f\"Critical failure in Stage 2: {e}\" # Add error info\n",
        "\n",
        "    # Return the original drafts and the final synthesized response (or None)\n",
        "    return stage1_results, final_response\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SCRAPING, SEARCHING, FINDING ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_links_and_snippets(search_results: List[Dict]) -> List[Dict[str, str]]:\n",
        "    logging.info(\"Extracting links and snippets from search results\")\n",
        "    extracted_data = []\n",
        "    for result in search_results:\n",
        "        data = {\n",
        "            'link': result.get('link') or result.get('formattedUrl'),\n",
        "            'snippet': result.get('snippet') or result.get('htmlSnippet', '')\n",
        "        }\n",
        "        extracted_data.append(data)\n",
        "    logging.info(f\"Extracted {len(extracted_data)} items from search results\")\n",
        "    return extracted_data\n",
        "\n",
        "def prepare_for_scraping_and_context(extracted_data: List[Dict[str, str]]) -> Tuple[List[str], List[Dict[str, str]]]:\n",
        "    logging.info(\"Preparing data for scraping and context\")\n",
        "    links_to_scrape = []\n",
        "    browser_context = []\n",
        "\n",
        "    for item in extracted_data:\n",
        "        links_to_scrape.append(item['link'])\n",
        "        browser_context.append({\n",
        "            'url': item['link'],\n",
        "            'snippet': item['snippet']\n",
        "        })\n",
        "\n",
        "    logging.info(f\"Prepared {len(links_to_scrape)} links for scraping\")\n",
        "    return links_to_scrape, browser_context\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_additional_info(name: str, company_name: str) -> str:\n",
        "    logging.info(f\"Getting additional info for {name} at {company_name}\")\n",
        "    query = f\"{name} {company_name}\"\n",
        "    search_results = FindKeyStakeholders.google_search_cached(query)\n",
        "    urls = [result['link'] for result in search_results[:5]]  # Get top 5 results\n",
        "    logging.info(f\"Scraping {len(urls)} URLs for additional info on {query}\")\n",
        "    scraped_content = cached_scrape_multiple(tuple(urls))\n",
        "    return \"\\n\".join(filter(None, scraped_content))\n",
        "\n",
        "def process_stakeholder(stakeholder: Dict[str, str], gather_additional_info: bool = False) -> str:\n",
        "    try:\n",
        "        name = stakeholder['name']\n",
        "        company_name = stakeholder['company_name']\n",
        "        role = stakeholder.get('role', 'Unknown Role')\n",
        "        email = stakeholder.get('email')\n",
        "\n",
        "        logging.info(f\"Processing stakeholder: {name} from {company_name}\")\n",
        "\n",
        "        if not email:\n",
        "            logging.info(f\"Generating email for {name}\")\n",
        "            email_util = TomsEmailUtilities(name, stakeholder['company_domain'])\n",
        "            valid_emails = email_util.email_generator()\n",
        "            if valid_emails:\n",
        "                email = valid_emails[0]\n",
        "                logging.info(f\"Generated email for {name}: {email}\")\n",
        "            else:\n",
        "                logging.warning(f\"No valid email found for {name}\")\n",
        "\n",
        "        primary_context = (f\"Name: {name}\\n\"\n",
        "                           f\"Company: {company_name}\\n\"\n",
        "                           f\"Role: {role}\\n\"\n",
        "                           f\"Email: {email if email else 'Not provided'}\\n\"\n",
        "                           f\"LinkedIn Snippet: {stakeholder['person_snippet']}\\n\"\n",
        "                           f\"Company Snippet: {stakeholder['company_snippet']}\")\n",
        "\n",
        "        if gather_additional_info:\n",
        "            logging.info(f\"Gathering additional information for {name}\")\n",
        "            additional_info = get_additional_info(name, company_name)\n",
        "            primary_context += f\"\\nAdditional Information: {additional_info}\"\n",
        "\n",
        "        # If email is found, automatically generate LLM answer and save all details\n",
        "        if email:\n",
        "            logging.info(f\"Email found for {name}: {email}, generating and saving all details.\")\n",
        "            intro_email = get_llm_answer('introductory_email', primary_context, tomides_profile)\n",
        "\n",
        "            # Collect and save all data in one text file\n",
        "            person_data = {\n",
        "                \"name\": name,\n",
        "                \"role\": role,\n",
        "                \"company_name\": company_name,\n",
        "                \"linkedin_snippet\": stakeholder['person_snippet'],\n",
        "                \"company_snippet\": stakeholder['company_snippet'],\n",
        "                \"email\": email,\n",
        "                \"search_cache\": FindKeyStakeholders.search_cache.get(f\"{name} {company_name}\", [])\n",
        "            }\n",
        "\n",
        "            # Prepare the content for the text file\n",
        "            file_content = (\n",
        "                f\"--- Person Details ---\\n\"\n",
        "                f\"Name: {person_data['name']}\\n\"\n",
        "                f\"Role: {person_data['role']}\\n\"\n",
        "                f\"Company: {person_data['company_name']}\\n\"\n",
        "                f\"LinkedIn Snippet: {person_data['linkedin_snippet']}\\n\"\n",
        "                f\"Company Snippet: {person_data['company_snippet']}\\n\"\n",
        "                f\"Email: {person_data['email']}\\n\\n\"\n",
        "                f\"--- Intro Email ---\\n\"\n",
        "                f\"{intro_email}\\n\\n\"\n",
        "                f\"--- Cached Search Results ---\\n\"\n",
        "                f\"{json.dumps(person_data['search_cache'], indent=4)}\"\n",
        "            )\n",
        "\n",
        "            # Save all the content into a single text file\n",
        "            filename = f\"{name.replace(' ', '_').lower()}_full_details.txt\"\n",
        "            save_response_to_file(file_content, filename)\n",
        "\n",
        "            logging.info(f\"Saved all details for {name} in {filename}\")\n",
        "\n",
        "            return f\"Generated and saved details for {name} in {filename}.\"\n",
        "        else:\n",
        "            # If no email is found, prompt the user\n",
        "            logging.warning(f\"Could not generate email for {name}. Prompting user for next action.\")\n",
        "            user_choice = input(f\"No email found for {name}. Would you still like to generate the LLM answer and save all details? (y/n): \").lower()\n",
        "\n",
        "            if user_choice == 'y':\n",
        "                logging.info(f\"User chose to generate the LLM answer for {name} despite no email.\")\n",
        "                intro_email = get_llm_answer('introductory_email', primary_context, tomides_profile)\n",
        "\n",
        "                # Collect and save all data in one text file\n",
        "                person_data = {\n",
        "                    \"name\": name,\n",
        "                    \"role\": role,\n",
        "                    \"company_name\": company_name,\n",
        "                    \"linkedin_snippet\": stakeholder['person_snippet'],\n",
        "                    \"company_snippet\": stakeholder['company_snippet'],\n",
        "                    \"email\": email,\n",
        "                    \"search_cache\": FindKeyStakeholders.search_cache.get(f\"{name} {company_name}\", [])\n",
        "                }\n",
        "\n",
        "                # Prepare the content for the text file\n",
        "                file_content = (\n",
        "                    f\"--- Person Details ---\\n\"\n",
        "                    f\"Name: {person_data['name']}\\n\"\n",
        "                    f\"Role: {person_data['role']}\\n\"\n",
        "                    f\"Company: {person_data['company_name']}\\n\"\n",
        "                    f\"LinkedIn Snippet: {person_data['linkedin_snippet']}\\n\"\n",
        "                    f\"Company Snippet: {person_data['company_snippet']}\\n\"\n",
        "                    f\"Email: No email found\\n\\n\"\n",
        "                    f\"--- Intro Email ---\\n\"\n",
        "                    f\"{intro_email}\\n\\n\"\n",
        "                    f\"--- Cached Search Results ---\\n\"\n",
        "                    f\"{json.dumps(person_data['search_cache'], indent=4)}\"\n",
        "                )\n",
        "\n",
        "                # Save all the content into a single text file\n",
        "                filename = f\"{name.replace(' ', '_').lower()}_full_details.txt\"\n",
        "                save_response_to_file(file_content, filename)\n",
        "\n",
        "                logging.info(f\"Saved all details for {name} in {filename}\")\n",
        "\n",
        "                return f\"Generated and saved details for {name} in {filename}.\"\n",
        "            else:\n",
        "                logging.info(f\"User chose not to generate LLM answer for {name}.\")\n",
        "                return f\"Could not generate email or save details for {name} due to lack of email address.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing stakeholder {stakeholder.get('name', 'Unknown')}: {str(e)}\", exc_info=True)\n",
        "        return f\"Error processing stakeholder {stakeholder.get('name', 'Unknown')}\"\n",
        "\n",
        "\n",
        "def automate_networking(companies: List[str], gather_additional_info: bool = False) -> None:\n",
        "    logging.info(f\"Starting networking automation for {len(companies)} companies\")\n",
        "\n",
        "    stakeholder_finder = FindKeyStakeholders()\n",
        "    all_stakeholders = stakeholder_finder.email_aggregator(companies)\n",
        "    logging.info(f\"Found {len(all_stakeholders)} stakeholders in total\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        logging.info(\"Submitting stakeholder processing tasks to ThreadPoolExecutor\")\n",
        "        futures = [executor.submit(process_stakeholder, stakeholder, gather_additional_info)\n",
        "                   for stakeholder in all_stakeholders]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            try:\n",
        "                result = future.result()\n",
        "                logging.info(f\"Processed stakeholder: {result}\")\n",
        "            except Exception as exc:\n",
        "                logging.error(f\"Error processing stakeholder: {exc}\", exc_info=True)\n",
        "\n",
        "def main() -> None:\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    companies_input = input(\"Enter comma-separated list of companies: \")\n",
        "    companies = [company.strip() for company in companies_input.split(',')]\n",
        "    logging.info(f\"Received input for {len(companies)} companies\")\n",
        "\n",
        "    gather_additional_info = input(\"Gather additional information for each stakeholder? (y/n): \").lower() == 'y'\n",
        "    logging.info(f\"Gather additional info: {gather_additional_info}\")\n",
        "\n",
        "    automate_networking(companies, gather_additional_info)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "APPLICATIONS (EMAIL OR QUESTION) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def automate_networking(companies: List[str], gather_additional_info: bool = False) -> None:\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    logging.info(f\"Starting networking automation for {len(companies)} companies\")\n",
        "\n",
        "    class FindKeyStakeholders:\n",
        "        back_up_list: List[Dict[str, Any]] = []\n",
        "        search_cache: Dict[str, Any] = {}\n",
        "\n",
        "        @staticmethod\n",
        "        def google_search_cached(query: str, use_api_first: bool = False) -> Optional[List[Dict[str, Any]]]:\n",
        "            logging.info(f\"Performing cached Google search for query: {query}\")\n",
        "            if query in FindKeyStakeholders.search_cache:\n",
        "                logging.info(f\"Query found in cache: {query}\")\n",
        "                return FindKeyStakeholders.search_cache[query]\n",
        "\n",
        "            logging.info(f\"Query not in cache, performing new search: {query}, use_api_first: {use_api_first}\")\n",
        "            response = google_search(query, fallback_to_api=True, use_api_first=use_api_first)\n",
        "\n",
        "            if response:\n",
        "                logging.info(f\"Search successful, caching results for query: {query}\")\n",
        "                FindKeyStakeholders.search_cache[query] = response\n",
        "            else:\n",
        "                logging.warning(f\"No results found for query: {query}\")\n",
        "\n",
        "            return response\n",
        "\n",
        "        @staticmethod\n",
        "        def getPersonInCompany(target_company: Dict[str, str], role: str, name: Dict[str, Optional[str]]) -> Optional[Dict[str, str]]:\n",
        "            try:\n",
        "                logging.info(f\"Searching for {role} at {target_company['company_name']}\")\n",
        "                query = f\"site:linkedin.com/in {role} {target_company['company_name']}\"\n",
        "\n",
        "                google_response = FindKeyStakeholders.google_search_cached(query, use_api_first=True)\n",
        "                logging.info(f\"Search result for {role} at {target_company['company_name']} - {google_response}\")\n",
        "\n",
        "                if not google_response:\n",
        "                    logging.warning(f\"No search results found for {role} at {target_company['company_name']}\")\n",
        "                    return None\n",
        "\n",
        "                target_linkedin_person = google_response[0]\n",
        "                logging.info(f\"Found potential LinkedIn profile for {role} at {target_company['company_name']}\")\n",
        "\n",
        "                linkedin_person_name = target_linkedin_person.get('title').replace(\"LinkedIn\", \"\").replace(\"|\", \"\").replace(\"-\", \"\").split()\n",
        "                logging.debug(f\"Extracted name from LinkedIn title: {linkedin_person_name}\")\n",
        "\n",
        "                first_name = name.get('first_name') or linkedin_person_name[0]\n",
        "                last_name = name.get('last_name') or (linkedin_person_name[1] if len(linkedin_person_name) > 1 else \"\")\n",
        "                full_name = f\"{first_name} {last_name}\"\n",
        "                logging.info(f\"Constructed full name: {full_name}\")\n",
        "\n",
        "                if any(target_linkedin_person['link'] == person['linkedin_url'] for person in FindKeyStakeholders.back_up_list):\n",
        "                    logging.info(f\"LinkedIn profile already in back-up list, skipping: {target_linkedin_person['link']}\")\n",
        "                    return None\n",
        "\n",
        "                logging.info(f\"Generating email for {full_name} at {target_company['company_domain']}\")\n",
        "                email = TomsEmailUtilities(full_name, target_company['company_domain']).email_generator()\n",
        "\n",
        "                person_data = {\n",
        "                    \"name\": full_name,\n",
        "                    \"role\": role,\n",
        "                    \"company_name\": target_company.get('company_name'),\n",
        "                    \"company_domain\": target_company.get('company_domain'),\n",
        "                    \"linkedin_url\": target_linkedin_person.get('link'),\n",
        "                    \"person_snippet\": target_linkedin_person.get('snippet'),\n",
        "                    \"company_snippet\": target_company.get('snippet'),\n",
        "                    \"email\": email\n",
        "                }\n",
        "\n",
        "                logging.info(f\"Person Data:\\n{json.dumps(person_data, indent=4)}\")\n",
        "\n",
        "                logging.info(f\"Successfully created person data for {full_name}\")\n",
        "                return person_data\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in getPersonInCompany: {str(e)}\", exc_info=True)\n",
        "                return None\n",
        "\n",
        "        @staticmethod\n",
        "        def get_company_details(company_name: str, key_stakeholders: List[str]) -> Optional[List[Dict[str, Any]]]:\n",
        "            try:\n",
        "                logging.info(f\"Getting company details for: {company_name}\")\n",
        "                google_response = FindKeyStakeholders.google_search_cached(f\"{company_name} company\")\n",
        "                if not google_response:\n",
        "                    logging.warning(f\"No search results found for company: {company_name}\")\n",
        "                    return None\n",
        "\n",
        "                exclude_list = [\"linkedin\", \"crunchbase\", \"pitchbook\", \"news\"]\n",
        "                companies = [\n",
        "                    {\n",
        "                        \"company_name\": company_name,\n",
        "                        \"title\": company.get('title', ''),\n",
        "                        \"link\": company.get('link', ''),\n",
        "                        \"snippet\": company.get('snippet', ''),\n",
        "                        \"company_domain\": company.get('link', ''),\n",
        "                        \"details\": [detail.get('snippet', '') for detail in google_response]\n",
        "                    }\n",
        "                    for company in google_response\n",
        "                    if not any(exclude in company['link'] for exclude in exclude_list)\n",
        "                ]\n",
        "\n",
        "                if not companies:\n",
        "                    logging.warning(f\"No valid company results found for: {company_name}\")\n",
        "                    return None\n",
        "\n",
        "                target_company = companies[0]\n",
        "                logging.info(f\"Full Company Data:\\n{json.dumps(target_company, indent=4)}\")\n",
        "\n",
        "                logging.info(f\"Selected target company: {target_company['company_name']}\")\n",
        "\n",
        "                name: Dict[str, Optional[str]] = {\"first_name\": None, \"last_name\": None}\n",
        "\n",
        "                persons = []\n",
        "                for role in key_stakeholders:\n",
        "                    logging.info(f\"Searching for {role} at {target_company['company_name']}\")\n",
        "                    person = FindKeyStakeholders.getPersonInCompany(target_company, role, name)\n",
        "                    if person:\n",
        "                        persons.append(person)\n",
        "                        FindKeyStakeholders.back_up_list.append(person)\n",
        "                        logging.info(f\"Added {role} to persons list: {person['name']}\")\n",
        "                    else:\n",
        "                        logging.info(f\"No {role} found for {target_company['company_name']}\")\n",
        "                return persons\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in get_company_details: {str(e)}\", exc_info=True)\n",
        "                return None\n",
        "\n",
        "        @staticmethod\n",
        "        def email_aggregator(target_company_list: List[str], key_stakeholders: List[str] = [\"PARTNER\", \"FOUNDER\", \"GENRRAL PARTNER\"]) -> List[Dict[str, Any]]:\n",
        "            logging.info(f\"Starting email aggregation for {len(target_company_list)} companies\")\n",
        "            responses = []\n",
        "            for company in target_company_list:\n",
        "                logging.info(f\"Processing company: {company}\")\n",
        "                persons = FindKeyStakeholders.get_company_details(company, key_stakeholders)\n",
        "                if persons:\n",
        "                    responses.extend(persons)\n",
        "                    logging.info(f\"Added {len(persons)} persons from {company}\")\n",
        "                else:\n",
        "                    logging.warning(f\"No persons found for company: {company}\")\n",
        "            logging.info(f\"Email aggregation complete. Total persons found: {len(responses)}\")\n",
        "            return responses\n",
        "\n",
        "    @lru_cache(maxsize=100)\n",
        "    def cached_scrape_multiple(urls_tuple: Tuple[str, ...]) -> List[str]:\n",
        "        logging.info(f\"Scraping multiple URLs (cached): {urls_tuple}\")\n",
        "        return scrape_multiple(list(urls_tuple), use_selenium=True, body_only=True, headless=True)\n",
        "\n",
        "    def get_additional_info(name: str, company_name: str) -> str:\n",
        "        logging.info(f\"Getting additional info for {name} at {company_name}\")\n",
        "        query = f\"{name} {company_name}\"\n",
        "        search_results = FindKeyStakeholders.google_search_cached(query)\n",
        "        urls = [result['link'] for result in search_results[:5]]  # Get top 5 results\n",
        "        logging.info(f\"Scraping {len(urls)} URLs for additional info on {query}\")\n",
        "        scraped_content = cached_scrape_multiple(tuple(urls))\n",
        "        return \"\\n\".join(filter(None, scraped_content))\n",
        "\n",
        "    def process_stakeholder(stakeholder: Dict[str, str], gather_additional_info: bool = False) -> str:\n",
        "        try:\n",
        "            name = stakeholder['name']\n",
        "            company_name = stakeholder['company_name']\n",
        "            role = stakeholder.get('role', 'Unknown Role')\n",
        "            email = stakeholder.get('email')\n",
        "\n",
        "            logging.info(f\"Processing stakeholder: {name} from {company_name}\")\n",
        "            primary_context = (f\"Name: {name}\\n\"\n",
        "                            f\"Company: {company_name}\\n\"\n",
        "                            f\"Role: {role}\\n\"\n",
        "                            f\"Email: {email if email else 'Not provided'}\\n\"\n",
        "                            f\"LinkedIn Snippet: {stakeholder['person_snippet']}\\n\"\n",
        "                            f\"Company Snippet: {stakeholder['company_snippet']}\")\n",
        "            if not email:\n",
        "                logging.info(f\"Generating email for {name}\")\n",
        "                email_util = TomsEmailUtilities(name, stakeholder['company_domain'])\n",
        "                valid_emails = email_util.email_generator()\n",
        "                if valid_emails:\n",
        "                    email = valid_emails[0]\n",
        "                    logging.info(f\"Generated email for {name}: {email}\")\n",
        "                else:\n",
        "                    logging.warning(f\"Could not generate email for {name}. Prompting user for next action.\")\n",
        "                    user_choice = input(f\"No email found for {name}. Would you still like to generate the LLM answer and save all details? (y/n): \").lower()\n",
        "\n",
        "                    if user_choice != 'y':\n",
        "                        logging.info(f\"User chose not to process {name}. Skipping.\")\n",
        "                        return f\"Skipped processing for {name}.\"\n",
        "\n",
        "            if gather_additional_info:\n",
        "                logging.info(f\"Gathering additional information for {name}\")\n",
        "                additional_info = get_additional_info(name, company_name)\n",
        "                primary_context += f\"\\nAdditional Information: {additional_info}\"\n",
        "\n",
        "            intro_email = get_llm_answer('introductory_email', primary_context, tomides_profile)\n",
        "            person_data = {\n",
        "                \"name\": name,\n",
        "                \"role\": role,\n",
        "                \"company_name\": company_name,\n",
        "                \"linkedin_snippet\": stakeholder['person_snippet'],\n",
        "                \"company_snippet\": stakeholder['company_snippet'],\n",
        "                \"email\": email,\n",
        "                \"search_cache\": FindKeyStakeholders.search_cache.get(f\"{name} {company_name}\", [])\n",
        "            }\n",
        "\n",
        "            file_content = (\n",
        "                f\"--- Person Details ---\\n\"\n",
        "                f\"Name: {person_data['name']}\\n\"\n",
        "                f\"Role: {person_data['role']}\\n\"\n",
        "                f\"Company: {person_data['company_name']}\\n\"\n",
        "                f\"LinkedIn Snippet: {person_data['linkedin_snippet']}\\n\"\n",
        "                f\"Company Snippet: {person_data['company_snippet']}\\n\"\n",
        "                f\"Email: {email if email else 'No email found'}\\n\\n\"\n",
        "                f\"--- Intro Email ---\\n\"\n",
        "                f\"{intro_email}\\n\\n\"\n",
        "                f\"--- Cached Search Results ---\\n\"\n",
        "                f\"{json.dumps(person_data['search_cache'], indent=4)}\"\n",
        "            )\n",
        "\n",
        "            filename = f\"{name.replace(' ', '_').lower()}_full_details.txt\"\n",
        "            save_response_to_file(file_content, filename)\n",
        "\n",
        "            logging.info(f\"Saved all details for {name} in {filename}\")\n",
        "\n",
        "            return f\"Generated and saved details for {name} in {filename}.\"\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing stakeholder {stakeholder.get('name', 'Unknown')}: {str(e)}\", exc_info=True)\n",
        "            return f\"Error processing stakeholder {stakeholder.get('name', 'Unknown')}\"\n",
        "\n",
        "    stakeholder_finder = FindKeyStakeholders()\n",
        "    all_stakeholders = stakeholder_finder.email_aggregator(companies)\n",
        "    logging.info(f\"Found {len(all_stakeholders)} stakeholders in total\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        logging.info(\"Submitting stakeholder processing tasks to ThreadPoolExecutor\")\n",
        "        futures = [executor.submit(process_stakeholder, stakeholder, gather_additional_info)\n",
        "                   for stakeholder in all_stakeholders]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            try:\n",
        "                result = future.result()\n",
        "                logging.info(f\"Processed stakeholder: {result}\")\n",
        "            except Exception as exc:\n",
        "                logging.error(f\"Error processing stakeholder: {exc}\", exc_info=True)\n",
        "\n",
        "companies_input = input(\"Enter comma-separated list of companies: \")\n",
        "companies = [company.strip() for company in companies_input.split(',')]\n",
        "logging.info(f\"Received input for {len(companies)} companies\")\n",
        "\n",
        "gather_additional_info = input(\"Gather additional information for each stakeholder? (y/n): \").lower() == 'y'\n",
        "logging.info(f\"Gather additional info: {gather_additional_info}\")\n",
        "automate_networking(companies, gather_additional_info)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "INTEGRATED APPLICATION SYSTEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_links_and_snippets(search_results: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Extract links and snippets from the search results.\n",
        "    Handles potential variations in key names.\n",
        "    \"\"\"\n",
        "    logging.info(\"Extracting links and snippets from search results\")\n",
        "    extracted_data = []\n",
        "    if not isinstance(search_results, list):\n",
        "         logging.warning(f\"Expected a list of search results, got {type(search_results)}. Skipping extraction.\")\n",
        "         return extracted_data\n",
        "\n",
        "    for result in search_results:\n",
        "         if not isinstance(result, dict):\n",
        "              logging.warning(f\"Skipping non-dict item in search results: {result}\")\n",
        "              continue\n",
        "         link = result.get('link') or result.get('formattedUrl')\n",
        "         snippet = result.get('snippet') or result.get('htmlSnippet', '')\n",
        "         if link: # Only include if a link was found\n",
        "             data = {'link': link, 'snippet': snippet}\n",
        "             extracted_data.append(data)\n",
        "         else:\n",
        "             logging.warning(f\"Skipping search result with no link found: {result.get('title', 'N/A')}\")\n",
        "\n",
        "    logging.info(f\"Extracted {len(extracted_data)} items with links from search results\")\n",
        "    return extracted_data\n",
        "\n",
        "def prep_url_list_for_scraping(extracted_data: List[Dict[str, str]]) -> List[str]:\n",
        "    \"\"\"Prepares a list of unique URLs for scraping.\"\"\"\n",
        "    logging.info(\"Preparing URLs for scraping\")\n",
        "    urls_to_scrape = list(set(item['link'] for item in extracted_data if item.get('link'))) # Use set for uniqueness\n",
        "    logging.info(f\"Prepared {len(urls_to_scrape)} unique URLs for scraping\")\n",
        "    return urls_to_scrape\n",
        "\n",
        "def search_and_extract_web_context(search_query: str, num_results: int = 5) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Performs web search, extracts key info, scrapes URLs, and returns combined context.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[str, str]: (combined_snippets, combined_scraped_content)\n",
        "    \"\"\"\n",
        "    logging.info(f\"Performing web search and extraction for query: '{search_query}'\")\n",
        "    combined_snippets = \"\"\n",
        "    combined_scraped_content = \"\"\n",
        "    try:\n",
        "        # Step 1: Search\n",
        "        # Ensure google_search returns a list of dicts or handles errors\n",
        "        search_results = google_search(search_query=search_query, num_results=num_results, fallback_to_api=True)\n",
        "        if not search_results:\n",
        "             logging.warning(\"Web search returned no results.\")\n",
        "             return \"\", \"\"\n",
        "\n",
        "        # Step 2: Extract links and snippets\n",
        "        extracted_data = extract_links_and_snippets(search_results)\n",
        "        if not extracted_data:\n",
        "            logging.warning(\"No links/snippets extracted from search results.\")\n",
        "            return \"\", \"\"\n",
        "\n",
        "        # Combine snippets\n",
        "        combined_snippets = \"\\n\".join([\n",
        "             f\"Source: {item.get('link', 'N/A')}\\nSnippet: {item.get('snippet', 'N/A')}\\n---\"\n",
        "             for item in extracted_data\n",
        "         ])\n",
        "\n",
        "        # Step 3: Prepare URLs for scraping\n",
        "        urls_to_scrape = prep_url_list_for_scraping(extracted_data)\n",
        "\n",
        "        # Step 4: Scrape content (handle potential errors)\n",
        "        # Ensure scrape_multiple returns a list of strings or handles errors\n",
        "        scraped_contents = scrape_multiple(urls_to_scrape, use_selenium=True, body_only=True, headless=True)\n",
        "        if scraped_contents:\n",
        "            combined_scraped_content = \"\\n\\n\".join(\n",
        "                 f\"--- Content from {url} ---\\n{content[:1500]}...\" # Limit length\n",
        "                 for url, content in zip(urls_to_scrape, scraped_contents) if content # Only include if content exists\n",
        "             )\n",
        "        else:\n",
        "             logging.warning(\"Scraping yielded no content.\")\n",
        "\n",
        "        logging.info(\"Successfully generated web context.\")\n",
        "        return combined_snippets, combined_scraped_content\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during web search/extraction for '{search_query}': {e}\", exc_info=True)\n",
        "        return combined_snippets, combined_scraped_content # Return whatever was gathered\n",
        "\n",
        "\n",
        "# --- Unified Input Function ---\n",
        "\n",
        "def get_unified_inputs() -> Optional[Dict[str, any]]:\n",
        "    \"\"\"Gets user input for various task types.\"\"\"\n",
        "    inputs = {}\n",
        "    all_request_types = [\n",
        "        'email', 'cover_letter', 'question', 'intro_email',\n",
        "        'mentorship_request', 'linkedin_message', 'networking_followup', 'quit'\n",
        "    ]\n",
        "    print(\"\\nAvailable request types:\")\n",
        "    for type_name in all_request_types[:-1]: # Exclude 'quit' from list display\n",
        "        print(f\"- {type_name}\")\n",
        "\n",
        "    inputs['request_type'] = input(f\"Enter request type or 'quit': \").strip().lower()\n",
        "\n",
        "    while inputs['request_type'] not in all_request_types:\n",
        "        logging.error(f\"Invalid request type '{inputs['request_type']}'. Please choose from the list above.\")\n",
        "        inputs['request_type'] = input(f\"Enter request type or 'quit': \").strip().lower()\n",
        "\n",
        "    if inputs['request_type'] == 'quit':\n",
        "        return None\n",
        "\n",
        "    # --- Context specific inputs ---\n",
        "    inputs['primary_context_input'] = \"\" # General holder for job details, name, etc.\n",
        "    inputs['question'] = \"\"\n",
        "\n",
        "    if inputs['request_type'] in ['email', 'cover_letter']:\n",
        "        inputs['primary_context_input'] = input(\"Enter the Job Details/Description: \")\n",
        "    elif inputs['request_type'] == 'question':\n",
        "        inputs['primary_context_input'] = input(\"Enter the Primary Context for the question (optional, press Enter to skip): \")\n",
        "        inputs['question'] = input(\"Enter your specific question: \")\n",
        "        while not inputs['question']:\n",
        "             logging.warning(\"Question cannot be empty for 'question' request type.\")\n",
        "             inputs['question'] = input(\"Enter your specific question: \")\n",
        "    elif inputs['request_type'] in ['intro_email', 'mentorship_request', 'linkedin_message']:\n",
        "        inputs['primary_context_input'] = input(\"Enter the Person's Full Name: \")\n",
        "        while not inputs['primary_context_input']:\n",
        "             logging.warning(\"Person's name cannot be empty.\")\n",
        "             inputs['primary_context_input'] = input(\"Enter the Person's Full Name: \")\n",
        "    elif inputs['request_type'] == 'networking_followup':\n",
        "        inputs['primary_context_input'] = input(\"Enter details about the networking interaction (e.g., event name, date, key discussion points): \")\n",
        "        # Optionally ask for person's name too if not in details\n",
        "        name_check = input(\"Also enter the person's name if not clear from details (optional): \").strip()\n",
        "        if name_check:\n",
        "             inputs['primary_context_input'] += f\"\\nPerson's Name: {name_check}\"\n",
        "\n",
        "\n",
        "    # --- Optional Web Search ---\n",
        "    do_search = input(\"Do you want to search online for additional context? (yes/no): \").strip().lower()\n",
        "    inputs['search'] = (do_search == 'yes' or do_search == 'y')\n",
        "    inputs['search_query'] = \"\"\n",
        "\n",
        "    if inputs['search']:\n",
        "        default_query = inputs['primary_context_input'] if inputs['primary_context_input'] else \"relevant context\"\n",
        "        query_prompt = f\"Enter search query (e.g., company name, person's name, topic) [default: '{default_query[:50]}...']: \"\n",
        "        inputs['search_query'] = input(query_prompt).strip()\n",
        "        if not inputs['search_query']:\n",
        "            inputs['search_query'] = default_query # Use default if user presses Enter\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "\n",
        "def process_request(inputs: Dict[str, any]) -> Tuple[Optional[Dict], Optional[str]]:\n",
        "    \"\"\"\n",
        "    Processes the user request, builds context, calls LLM, and returns results.\n",
        "    \"\"\"\n",
        "    request_type = inputs['request_type']\n",
        "    primary_context_input = inputs['primary_context_input']\n",
        "    profile_context = tomides_profile # Use the globally loaded profile\n",
        "    question_text = inputs['question']\n",
        "    browser_context_snippets = \"\"\n",
        "    browser_context_scraped = \"\"\n",
        "    final_primary_context = \"\"\n",
        "\n",
        "    logging.info(f\"Processing request type: {request_type}\")\n",
        "\n",
        "    # 1. Perform Web Search if requested\n",
        "    if inputs['search'] and inputs['search_query']:\n",
        "        logging.info(f\"Initiating web search for query: '{inputs['search_query']}'\")\n",
        "        browser_context_snippets, browser_context_scraped = search_and_extract_web_context(inputs['search_query'])\n",
        "    elif inputs['request_type'] in ['intro_email', 'mentorship_request', 'linkedin_message'] and primary_context_input:\n",
        "         # Special case: Always search for the person if it's a person-focused task, even if user didn't explicitly ask\n",
        "         logging.info(f\"Implicitly searching web for person: '{primary_context_input}' for request type {request_type}\")\n",
        "         browser_context_snippets, browser_context_scraped = search_and_extract_web_context(primary_context_input)\n",
        "\n",
        "\n",
        "    # 2. Construct Final Primary Context based on request type\n",
        "    if request_type in ['email', 'cover_letter']:\n",
        "        final_primary_context = f\"Job Context:\\n{primary_context_input}\"\n",
        "    elif request_type == 'question':\n",
        "         final_primary_context = f\"Question Context:\\n{primary_context_input if primary_context_input else 'General Knowledge'}\"\n",
        "    elif request_type in ['intro_email', 'mentorship_request', 'linkedin_message']:\n",
        "         final_primary_context = f\"Regarding Person: {primary_context_input}\"\n",
        "         # Optional: Add snippets directly to primary context if helpful? Let's keep them in browser context for now.\n",
        "    elif request_type == 'networking_followup':\n",
        "         final_primary_context = f\"Networking Follow-up Context:\\n{primary_context_input}\"\n",
        "    else: # Fallback if new types added without explicit handling\n",
        "         final_primary_context = primary_context_input\n",
        "\n",
        "\n",
        "    # 3. Combine Browser Context\n",
        "    # We pass snippets and scraped content separately to the LLM function if needed,\n",
        "    # or combine them here. Let's combine for simplicity in this example.\n",
        "    # The LLM prompt needs to know how to use this combined context.\n",
        "    combined_browser_context = \"\"\n",
        "    if browser_context_snippets:\n",
        "        combined_browser_context += f\"--- Relevant Web Snippets ---\\n{browser_context_snippets}\\n\\n\"\n",
        "    if browser_context_scraped:\n",
        "        combined_browser_context += f\"--- Content from Web Pages ---\\n{browser_context_scraped}\"\n",
        "\n",
        "    # 4. Call the LLM\n",
        "    logging.info(\"Calling LLM for final answer generation...\")\n",
        "    try:\n",
        "        # Assuming get_llm_answer takes browser_context as a single string now\n",
        "        # Modify get_llm_answer's prompt to handle this combined context effectively\n",
        "        stage1_results, final_answer = get_llm_answer(\n",
        "            request_type=request_type,\n",
        "            primary_context=final_primary_context,\n",
        "            profile_context=profile_context,\n",
        "            browser_context=combined_browser_context if combined_browser_context else None, # Pass combined context\n",
        "            question=question_text\n",
        "            # Add generation_providers, timeouts if needed\n",
        "        )\n",
        "        logging.info(\"LLM call completed.\")\n",
        "        return stage1_results, final_answer\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error calling get_llm_answer: {e}\", exc_info=True)\n",
        "        return {\"error\": f\"LLM processing failed: {e}\"}, None\n",
        "\n",
        "\n",
        "# --- Unified Main Execution Logic ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main loop to get inputs and process requests.\"\"\"\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    logging.info(\"Application started. Loading profile...\")\n",
        "    # Profile is loaded globally at the start\n",
        "\n",
        "    while True:\n",
        "        inputs = get_unified_inputs()\n",
        "        if inputs is None:\n",
        "            logging.info(\"Quit signal received. Exiting.\")\n",
        "            break\n",
        "\n",
        "        stage1_results, final_answer = process_request(inputs)\n",
        "\n",
        "        # --- Output and Saving ---\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"Processed Request Type: {inputs['request_type']}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        if stage1_results:\n",
        "             print(\"\\n--- Stage 1 Drafts (Snippets) ---\")\n",
        "             for key, value in stage1_results.items():\n",
        "                 # Limit output length for readability\n",
        "                 snippet = str(value)[:300] + ('...' if len(str(value)) > 300 else '')\n",
        "                 print(f\"[{key}]:\\n{snippet}\\n\")\n",
        "        else:\n",
        "             print(\"\\n--- Stage 1 Drafts: None generated ---\")\n",
        "\n",
        "\n",
        "        print(\"\\n--- Stage 2 Final Answer ---\")\n",
        "        if final_answer:\n",
        "            print(final_answer)\n",
        "        else:\n",
        "            print(\"No final answer generated or synthesis failed.\")\n",
        "            if stage1_results and stage1_results.get('synthesis_error'):\n",
        "                 print(f\"(Synthesis Error: {stage1_results['synthesis_error']})\")\n",
        "            elif stage1_results and stage1_results.get('error'):\n",
        "                 print(f\"(Stage 1 Error: {stage1_results['error']})\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "        # --- Saving ---\n",
        "        base_filename = inputs['request_type']\n",
        "        if inputs['request_type'] in ['intro_email', 'mentorship_request', 'linkedin_message'] and inputs['primary_context_input']:\n",
        "            # Use person's name in filename\n",
        "             name_part = inputs['primary_context_input'].split('\\n')[0].replace('Regarding Person: ', '').strip()\n",
        "             safe_name = name_part.replace(' ', '_').lower()\n",
        "             base_filename = f\"{safe_name}_{inputs['request_type']}\"\n",
        "        elif inputs['request_type'] == 'question' and inputs['question']:\n",
        "             # Use part of the question in filename\n",
        "             q_part = inputs['question'][:30].replace(' ', '_').lower()\n",
        "             safe_q = ''.join(c for c in q_part if c.isalnum() or c == '_')\n",
        "             base_filename = f\"question_{safe_q}_{inputs['request_type']}\"\n",
        "\n",
        "        # Prepare content to save (example: combine both stages)\n",
        "        save_content = f\"Request Type: {inputs['request_type']}\\n\"\n",
        "        save_content += f\"Primary Context Input: {inputs['primary_context_input']}\\n\"\n",
        "        save_content += f\"Question (if any): {inputs['question']}\\n\"\n",
        "        save_content += f\"Search Performed: {inputs['search']}\\n\"\n",
        "        save_content += f\"Search Query: {inputs['search_query']}\\n\"\n",
        "        save_content += \"\\n\" + \"=\"*20 + \" Stage 1 Results \" + \"=\"*20 + \"\\n\\n\"\n",
        "        save_content += json.dumps(stage1_results, indent=2) # Save stage 1 results as JSON\n",
        "        save_content += \"\\n\\n\" + \"=\"*20 + \" Stage 2 Final Answer \" + \"=\"*20 + \"\\n\\n\"\n",
        "        save_content += final_answer if final_answer else \"No final answer generated.\"\n",
        "\n",
        "        try:\n",
        "             save_response_to_file(save_content, base_filename)\n",
        "             logging.info(f\"Results saved successfully.\") # Assuming save_response handles filename creation\n",
        "        except Exception as e:\n",
        "             logging.error(f\"Failed to save results: {e}\")\n",
        "\n",
        "\n",
        "        print(\"\\n\" + \"-\"*50 + \"\\n\") # Separator for next loop iteration\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    def save_response_to_file(content, filename_prefix):\n",
        "        filename = f\"{filename_prefix}_{time.strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "        try:\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(content)\n",
        "            logging.info(f\"Placeholder: Content saved to {filename}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Placeholder save failed: {e}\")\n",
        "\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Gemini_LangChain_QA_Chroma_WebLoad.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
